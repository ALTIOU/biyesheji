#!/usr/bin/env python3
"""
æµ‹è¯• SFT è®­ç»ƒç¯å¢ƒæ˜¯å¦é…ç½®æ­£ç¡®
"""

import os
import sys
import json
from pathlib import Path

def test_imports():
    """æµ‹è¯•å¿…è¦çš„åŒ…æ˜¯å¦èƒ½å¯¼å…¥"""
    print("ğŸ”§ æµ‹è¯•åŒ…å¯¼å…¥...")
    try:
        import torch
        print(f"âœ… torch ç‰ˆæœ¬: {torch.__version__}")
        print(f"   CUDA å¯ç”¨: {torch.cuda.is_available()}")

        import transformers
        print(f"âœ… transformers ç‰ˆæœ¬: {transformers.__version__}")

        import peft
        print(f"âœ… peft ç‰ˆæœ¬: {peft.__version__}")

        import accelerate
        print(f"âœ… accelerate ç‰ˆæœ¬: {accelerate.__version__}")

        import datasets
        print(f"âœ… datasets ç‰ˆæœ¬: {datasets.__version__}")

        return True
    except ImportError as e:
        print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        return False

def test_data_loading():
    """æµ‹è¯•æ•°æ®åŠ è½½"""
    print("\nğŸ“ æµ‹è¯•æ•°æ®åŠ è½½...")
    data_path = "../data/raw/dataset_prepared/sft_data.jsonl"

    if not os.path.exists(data_path):
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
        return False

    try:
        with open(data_path, "r", encoding="utf-8") as f:
            count = 0
            for line in f:
                item = json.loads(line)
                if count < 2:  # åªæ˜¾ç¤ºå‰2ä¸ªæ ·æœ¬
                    print(f"   ç¤ºä¾‹ {count+1}: {item['instruction'][:50]}...")
                count += 1
                if count >= 10:  # åªè¯»å–å‰10ä¸ª
                    break

        print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸï¼Œå…± {count} æ¡è®°å½•")
        return True
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return False

def test_tokenizer():
    """æµ‹è¯• tokenizer åŠ è½½"""
    print("\nğŸ”¤ æµ‹è¯• tokenizer...")
    try:
        from transformers import AutoTokenizer

        tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen3-1.7B", trust_remote_code=True)
        print("âœ… Tokenizer åŠ è½½æˆåŠŸ")

        # æµ‹è¯•ç¼–ç 
        test_text = "ä½ å¥½ï¼Œè¿™æ˜¯æµ‹è¯•æ–‡æœ¬ã€‚"
        tokens = tokenizer.encode(test_text)
        print(f"âœ… ç¼–ç æµ‹è¯•: '{test_text}' -> {len(tokens)} tokens")

        return True
    except Exception as e:
        print(f"âŒ Tokenizer æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹ SFT è®­ç»ƒç¯å¢ƒæµ‹è¯•\n")

    results = []
    results.append(("åŒ…å¯¼å…¥", test_imports()))
    results.append(("æ•°æ®åŠ è½½", test_data_loading()))
    results.append(("Tokenizer", test_tokenizer()))

    print("\n" + "="*50)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“:")

    all_passed = True
    for test_name, passed in results:
        status = "âœ… é€šè¿‡" if passed else "âŒ å¤±è´¥"
        print(f"   {test_name}: {status}")
        all_passed = all_passed and passed

    if all_passed:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç¯å¢ƒé…ç½®æ­£ç¡®ï¼Œå¯ä»¥å¼€å§‹è®­ç»ƒã€‚")
        print("\nè¿è¡Œè®­ç»ƒå‘½ä»¤:")
        print("cd /Users/altiou/code_learn/æ¯•ä¸šè®¾è®¡")
        print("conda activate biyesheji")
        print("python project/src/sft/train_sft_lora.py")
    else:
        print("\nâš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç¯å¢ƒé…ç½®ã€‚")

    return all_passed

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
