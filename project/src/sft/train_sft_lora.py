#!/usr/bin/env python3
"""
SFT (Supervised Fine-Tuning) è®­ç»ƒè„šæœ¬

ç›®å½•ç»“æ„ï¼š
- æ¯æ¬¡è®­ç»ƒä¼šåœ¨ models/sft/ ä¸‹åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„å­ç›®å½•ï¼šsft_YYYYMMDD_HHMMSS/
- åŒ…å«å®Œæ•´çš„æ¨¡å‹æ–‡ä»¶å’Œè®­ç»ƒå…ƒæ•°æ®

ç¯å¢ƒè¯´æ˜ï¼š
- M2 Mac æµ‹è¯•ç¯å¢ƒï¼šå½“å‰æ¿€æ´»é…ç½®ï¼Œä¼˜åŒ–å†…å­˜ä½¿ç”¨
- GPU æ‰¹é‡å®éªŒç¯å¢ƒï¼šæ³¨é‡Šæ‰çš„é…ç½®ï¼Œæ›´é«˜æ•ˆç‡

è¿è¡Œæ–¹å¼ï¼š
1. M2 Mac æµ‹è¯•ï¼ˆå·²å¯ç”¨ WandBï¼‰ï¼š
   cd project/mac_test
   conda activate biyesheji
   python ../src/sft/train_sft_lora.py

2. GPU æ‰¹é‡å®éªŒï¼š
   - å–æ¶ˆæ³¨é‡Š GPU é…ç½®éƒ¨åˆ†
   - æ³¨é‡Šæ‰ Mac é…ç½®éƒ¨åˆ†
   - è°ƒæ•´è®­ç»ƒå‚æ•°ï¼ˆepochs, batch_sizeç­‰ï¼‰
   - ç¡®ä¿ WandB å·²ç™»å½•

WandB é¡¹ç›®ï¼šsft_qwen3_lora
"""

import os
import json
from datetime import datetime
import wandb
from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, BitsAndBytesConfig
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

# è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œé¿å… OMP å†²çªï¼ˆMac ç¯å¢ƒéœ€è¦ï¼‰
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

# WandB é…ç½®
WANDB_PROJECT = "sft_qwen3_lora"
wandb.init(project=WANDB_PROJECT)

# è·¯å¾„é…ç½®
SFT_DATA_PATH = "project/data/processed/sft_data.jsonl"
BASE_MODEL = "Qwen/Qwen3-1.7B"

# åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„è¾“å‡ºç›®å½•
timestamp = datetime.now().strftime("%Y%m%d_%H%M")
OUTPUT_DIR = f"project/models/sft/sft_{timestamp}"
os.makedirs(OUTPUT_DIR, exist_ok=True)

def load_sft_dataset(path):
    data_list = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            item = json.loads(line)
            prompt = item["instruction"] + (("\n" + item["input"]) if item["input"] else "")
            text = prompt + "\n\n" + item["output"]
            data_list.append({"text": text})
    return data_list

def tokenize(example):
    # M2 Mac æµ‹è¯•é…ç½® - ä½¿ç”¨è¾ƒçŸ­çš„åºåˆ—é•¿åº¦ä»¥èŠ‚çœå†…å­˜
    result = tokenizer(example["text"], truncation=True, max_length=512)
    result["labels"] = result["input_ids"].copy()  # æ·»åŠ  labels ç”¨äºè®¡ç®— loss
    return result

    # æ‰¹é‡å®éªŒ GPU é…ç½®ï¼ˆæ³¨é‡Šæ‰ï¼‰ï¼š
    # result = tokenizer(example["text"], truncation=True, max_length=1024)  # GPU å¯ä»¥å¤„ç†æ›´é•¿åºåˆ—
    # result["labels"] = result["input_ids"].copy()
    # return result

if __name__ == "__main__":
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    print("ğŸ”§ Loading tokenizer & model...")
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)
    # M2 Mac æµ‹è¯•é…ç½® - CPU è®­ç»ƒï¼Œä¸ä½¿ç”¨é‡åŒ–ä»¥ç¡®ä¿å…¼å®¹æ€§
    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        trust_remote_code=True,
        # device_map="auto",  # CPU è®­ç»ƒä¸éœ€è¦
        torch_dtype="float32"  # CPU æ¨¡å¼ä½¿ç”¨ float32
    )

    # æ‰¹é‡å®éªŒ GPU é…ç½®ï¼ˆæ³¨é‡Šæ‰ï¼‰ï¼š
    # model = AutoModelForCausalLM.from_pretrained(
    #     BASE_MODEL,
    #     trust_remote_code=True,
    #     device_map="auto",
    # )
    
    print("ğŸ”§ Preparing LoRA config...")
    # M2 Mac æµ‹è¯•é…ç½® - ä½¿ç”¨è¾ƒå°çš„ r å€¼ä»¥èŠ‚çœå†…å­˜
    lora_config = LoraConfig(
        r=8,  # å‡å° r å€¼ï¼ŒèŠ‚çœå†…å­˜
        lora_alpha=16,  # ç›¸åº”è°ƒæ•´ alpha
        target_modules=["q_proj","v_proj"],
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM"
    )

    # æ‰¹é‡å®éªŒ GPU é…ç½®ï¼ˆæ³¨é‡Šæ‰ï¼‰ï¼š
    # lora_config = LoraConfig(
    #     r=16,  # GPU ç¯å¢ƒå¯ä»¥ä½¿ç”¨æ›´å¤§çš„ r å€¼
    #     lora_alpha=32,
    #     target_modules=["q_proj","v_proj"],
    #     lora_dropout=0.05,
    #     bias="none",
    #     task_type="CAUSAL_LM"
    # )

    # M2 Mac CPU æ¨¡å¼ - ä¸éœ€è¦ prepare_model_for_kbit_training
    model = get_peft_model(model, lora_config)

    print("ğŸ“¥ Loading SFT dataset...")
    dataset_list = load_sft_dataset(SFT_DATA_PATH)

    # M2 Mac æµ‹è¯•é…ç½® - ä½¿ç”¨ç®€å•çš„æ•°æ®æ ¼å¼
    print(f"   åŠ è½½äº† {len(dataset_list)} æ¡è®­ç»ƒæ•°æ®")

    # åªå¤„ç†å°‘é‡æ•°æ®è¿›è¡Œæµ‹è¯•
    test_dataset_list = dataset_list[:50]  # åªç”¨å‰10æ¡è¿›è¡Œæµ‹è¯•
    print(f"   æµ‹è¯•æ¨¡å¼ï¼šä½¿ç”¨ {len(test_dataset_list)} æ¡æ•°æ®")

    tokenized_data = []
    for item in test_dataset_list:
        tokenized_item = tokenize(item)
        tokenized_data.append(tokenized_item)

    # åˆ›å»ºç®€å•çš„ Dataset ç±»
    class SimpleDataset:
        def __init__(self, data):
            self.data = data

        def __len__(self):
            return len(self.data)

        def __getitem__(self, idx):
            return self.data[idx]

    train_dataset = SimpleDataset(tokenized_data)

    print("ğŸš€ Starting SFT training...")
    # M2 Mac æµ‹è¯•é…ç½® - å¯ç”¨ WandB è®°å½•
    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        per_device_train_batch_size=1,  # Mac å†…å­˜é™åˆ¶ï¼Œä½¿ç”¨å° batch
        gradient_accumulation_steps=4,  # å‡å°‘ç´¯ç§¯æ­¥æ•°ï¼ŒåŠ å¿«è®­ç»ƒ
        num_train_epochs=1,  # æµ‹è¯•ç”¨ï¼Œåªè®­ç»ƒ 1 è½®
        logging_steps=1,  # æ›´é¢‘ç¹çš„æ—¥å¿—è¾“å‡º
        save_strategy="epoch",
        learning_rate=2e-4,
        # fp16=True,  # M2 Mac ä¸æ”¯æŒ fp16
        bf16=False,  # M2 Mac CPU æ¨¡å¼ä¸‹ç¦ç”¨ bf16
        report_to="wandb",  # å¯ç”¨ WandB è®°å½•
        run_name=f"sft_{timestamp}",  # æ¯æ¬¡è¿è¡Œçš„å”¯ä¸€åç§°
        # æ˜ç¡®æŒ‡å®š CPU è®­ç»ƒï¼Œé¿å… accelerate è®¾å¤‡æ£€æµ‹é—®é¢˜
        no_cuda=True,
        dataloader_num_workers=0  # CPU è®­ç»ƒæ—¶é¿å…å¤šè¿›ç¨‹é—®é¢˜
    )

    # æ‰¹é‡å®éªŒ GPU é…ç½®ï¼ˆæ³¨é‡Šæ‰ï¼‰ï¼š
    # training_args = TrainingArguments(
    #     output_dir=OUTPUT_DIR,
    #     per_device_train_batch_size=4,  # GPU å¯ä»¥ç”¨æ›´å¤§ batch
    #     gradient_accumulation_steps=8,
    #     num_train_epochs=3,  # æ­£å¼è®­ç»ƒç”¨ 3 è½®
    #     logging_steps=20,
    #     save_strategy="epoch",
    #     learning_rate=2e-4,
    #     fp16=True,  # GPU æ”¯æŒ fp16ï¼Œæ•ˆç‡æ›´é«˜
    #     report_to="wandb",  # GPU ç¯å¢ƒä½¿ç”¨ WandB è®°å½•
    #     run_name=f"sft_gpu_{timestamp}"
    # )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        tokenizer=tokenizer
    )

    trainer.train()

    print("ğŸ’¾ Saving LoRA SFT model...")
    model.save_pretrained(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)

    # è·å–è®­ç»ƒç»“æœ
    final_loss = trainer.state.log_history[-1].get("train_loss") if trainer.state.log_history else None

    # åˆ›å»ºè®­ç»ƒä¿¡æ¯è®°å½•æ–‡ä»¶
    metadata = {
        "training_timestamp": timestamp,
        "training_datetime": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "base_model": BASE_MODEL,
        "data_path": SFT_DATA_PATH,
        "training_config": {
            "epochs": training_args.num_train_epochs,
            "batch_size": training_args.per_device_train_batch_size,
            "gradient_accumulation": training_args.gradient_accumulation_steps,
            "learning_rate": training_args.learning_rate,
            "max_length": 512,  # tokenize å‡½æ•°ä¸­çš„å€¼
            "lora_r": lora_config.r,
            "lora_alpha": lora_config.lora_alpha,
        },
        "final_loss": final_loss,
        "total_steps": trainer.state.global_step,
        "output_directory": OUTPUT_DIR,
        "files_saved": [
            "adapter_config.json",
            "adapter_model.safetensors",
            "tokenizer.json",
            "tokenizer_config.json",
            "special_tokens_map.json",
            "vocab.json",
            "merges.txt",
            "added_tokens.json",
            "chat_template.jinja",
            "training_metadata.json"
        ]
    }

    metadata_path = os.path.join(OUTPUT_DIR, "training_metadata.json")
    with open(metadata_path, "w", encoding="utf-8") as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)

    # è®°å½•åˆ° WandB
    wandb.log({
        "final_loss": final_loss,
        "total_steps": trainer.state.global_step,
        "training_runtime": trainer.state.log_history[-1].get("train_runtime") if trainer.state.log_history else None,
        "training_config": metadata["training_config"]
    })

    # æ·»åŠ æ ‡ç­¾å’Œæè¿°
    wandb.run.tags = ["sft", "qwen3", "lora", "mac_test"]
    wandb.run.notes = f"SFT training with Qwen3-1.7B on Mac M2. Final loss: {final_loss:.4f}"

    print(f"ğŸ“ Training metadata saved to: {metadata_path}")
    print(f"ğŸ“Š WandB run URL: {wandb.run.url}")
    print("ğŸ‰ SFT training complete!")
