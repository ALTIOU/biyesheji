#!/usr/bin/env python3
"""
SFT (Supervised Fine-Tuning) è®­ç»ƒè„šæœ¬

ç›®å½•ç»“æ„ï¼š
- æ¯æ¬¡è®­ç»ƒä¼šåœ¨ models/sft/ ä¸‹åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„å­ç›®å½•ï¼šsft_YYYYMMDD_HHMMSS/
- åŒ…å«å®Œæ•´çš„æ¨¡å‹æ–‡ä»¶å’Œè®­ç»ƒå…ƒæ•°æ®

ç¯å¢ƒè¯´æ˜ï¼š
- M2 Mac æµ‹è¯•ç¯å¢ƒï¼šå½“å‰æ¿€æ´»é…ç½®ï¼Œä¼˜åŒ–å†…å­˜ä½¿ç”¨
- GPU æ‰¹é‡å®éªŒç¯å¢ƒï¼šæ³¨é‡Šæ‰çš„é…ç½®ï¼Œæ›´é«˜æ•ˆç‡

è¿è¡Œæ–¹å¼ï¼š
1. M2 Mac æµ‹è¯•ï¼ˆå·²å¯ç”¨ WandBï¼‰ï¼š
   cd project/mac_test
   conda activate biyesheji
   python ../src/sft/train_sft_lora.py

2. GPU æ‰¹é‡å®éªŒï¼š
   - å–æ¶ˆæ³¨é‡Š GPU é…ç½®éƒ¨åˆ†
   - æ³¨é‡Šæ‰ Mac é…ç½®éƒ¨åˆ†
   - è°ƒæ•´è®­ç»ƒå‚æ•°ï¼ˆepochs, batch_sizeç­‰ï¼‰
   - ç¡®ä¿ WandB å·²ç™»å½•

WandB é¡¹ç›®ï¼šsft_qwen3_lora
"""

import os
import json
from datetime import datetime
import argparse
from pathlib import Path
import warnings

import torch

from datasets import Dataset
from transformers import (
    AutoConfig,
    AutoModelForCausalLM,
    AutoTokenizer,
    DataCollatorForLanguageModeling,
    Trainer,
    TrainingArguments,
)
from peft import LoraConfig, get_peft_model

# è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œé¿å… OMP å†²çªï¼ˆMac ç¯å¢ƒéœ€è¦ï¼‰
# os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

WANDB_PROJECT = "sft_qwen3_lora"

def load_sft_dataset(path):
    data_list = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            item = json.loads(line)
            prompt = item["instruction"] + (("\n" + item["input"]) if item["input"] else "")
            text = prompt + "\n\n" + item["output"]
            data_list.append({"text": text})
    return data_list

def build_arg_parser():
    parser = argparse.ArgumentParser(description="SFT (LoRA) è®­ç»ƒè„šæœ¬")
    parser.add_argument("--data_path", type=str, default="project/data/processed/sft_data.jsonl")
    parser.add_argument("--base_model", type=str, default="Qwen/Qwen3-1.7B")
    parser.add_argument("--output_dir", type=str, default=None, help="é»˜è®¤ä½¿ç”¨ project/models/sft/sft_<timestamp>")
    parser.add_argument("--max_length", type=int, default=1024)
    parser.add_argument("--max_train_samples", type=int, default=300, help="æµ‹è¯•ç”¨ï¼šé™åˆ¶è®­ç»ƒæ ·æœ¬æ•°ï¼›<=0 è¡¨ç¤ºä¸é™åˆ¶")
    parser.add_argument("--max_steps", type=int, default=0, help=">0 æ—¶è¦†ç›– epochsï¼ŒæŒ‰ step è®­ç»ƒï¼ˆé€‚åˆå†’çƒŸ/å°è·‘ï¼‰")

    parser.add_argument("--per_device_train_batch_size", type=int, default=4)
    parser.add_argument("--gradient_accumulation_steps", type=int, default=8)
    parser.add_argument("--num_train_epochs", type=int, default=3)
    parser.add_argument("--learning_rate", type=float, default=2e-4)
    parser.add_argument("--logging_steps", type=int, default=20)
    parser.add_argument("--save_strategy", type=str, default="epoch", choices=["no", "steps", "epoch"])
    parser.add_argument("--gradient_checkpointing", action="store_true", help="å¼€å¯ä»¥æ˜¾è‘—é™ä½æ˜¾å­˜å ç”¨ï¼ˆæ¨è GPUï¼‰")

    parser.add_argument("--bf16", action="store_true", help="A100 æ¨è bf16ï¼ˆé»˜è®¤å¼€å¯ï¼‰")
    parser.add_argument("--fp16", action="store_true", help="å¦‚æœä¸æ”¯æŒ bf16ï¼Œå¯ç”¨ fp16")

    parser.add_argument("--report_to", type=str, default="wandb", choices=["none", "wandb"])
    parser.add_argument("--wandb_project", type=str, default=WANDB_PROJECT)
    parser.add_argument("--run_name", type=str, default=None)

    # LoRA è¶…å‚ï¼ˆä¸ºäº†æ”¯æŒ tiny æ¨¡å‹å†’çƒŸè·‘é€šï¼Œä»¥åŠåç»­å¿«é€Ÿè°ƒå‚ï¼‰
    parser.add_argument("--lora_r", type=int, default=16)
    parser.add_argument("--lora_alpha", type=int, default=32)
    parser.add_argument("--lora_dropout", type=float, default=0.05)
    parser.add_argument(
        "--lora_target_modules",
        type=str,
        default="q_proj,v_proj",
        help="é€—å·åˆ†éš”ã€‚ä¾‹å¦‚ Qwen: q_proj,v_projï¼›GPT2: c_attn",
    )
    parser.add_argument(
        "--init_from_config",
        action="store_true",
        help="ä»…ä» config éšæœºåˆå§‹åŒ–æ¨¡å‹ï¼ˆä¸åŠ è½½æƒé‡ï¼‰ã€‚ç”¨äºæ— å¡/ä½å†…å­˜/torch.load å—é™ç¯å¢ƒå†’çƒŸè·‘é€šã€‚",
    )
    return parser

if __name__ == "__main__":
    args = build_arg_parser().parse_args()

    # ========= è®¾å¤‡èƒ½åŠ›æ¨æ–­ï¼šæ—  CUDA æ—¶ä¸è¦é»˜è®¤å¼€å¯ bf16/fp16ï¼ˆæ— å¡/å°å†…å­˜ç¯å¢ƒå¾ˆå®¹æ˜“ç›´æ¥æŠ¥é”™ï¼‰ =========
    has_cuda = torch.cuda.is_available()
    if not args.bf16 and not args.fp16:
        # ä»…åœ¨ CUDA å¯ç”¨æ—¶é»˜è®¤å¯ç”¨ bf16ï¼ˆA100 ç­‰ï¼‰
        args.bf16 = bool(has_cuda)
    if not has_cuda and (args.bf16 or args.fp16):
        warnings.warn("æ£€æµ‹åˆ°æ—  CUDAï¼ˆæ— å¡ï¼‰ç¯å¢ƒï¼šå·²è‡ªåŠ¨å…³é—­ bf16/fp16 ä»¥é¿å…è¿è¡Œæ—¶æŠ¥é”™ã€‚", stacklevel=2)
        args.bf16 = False
        args.fp16 = False

    timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    output_dir = args.output_dir or f"project/models/sft/sft_{timestamp}"
    os.makedirs(output_dir, exist_ok=True)

    # WandBï¼šåªåœ¨éœ€è¦æ—¶åˆå§‹åŒ–ï¼Œé¿å…é›†ç¾¤æœªç™»å½•/ç¦»çº¿ç›´æ¥æŠ¥é”™
    wandb_run = None
    if args.report_to == "wandb":
        import wandb  # noqa: PLC0415

        wandb_run = wandb.init(project=args.wandb_project, name=(args.run_name or f"sft_{timestamp}"))

    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print("ğŸ”§ Loading tokenizer & model...")
    # HF é‰´æƒï¼šé•œåƒ/Hub é™æµæˆ–ç§æœ‰æ¨¡å‹æ—¶éœ€è¦ tokenï¼›æ”¯æŒ HF_TOKEN/HUGGINGFACE_HUB_TOKEN
    hf_token = os.environ.get("HF_TOKEN") or os.environ.get("HUGGINGFACE_HUB_TOKEN")
    if not hf_token:
        token_file = os.environ.get("HF_TOKEN_FILE")
        token_candidates = [
            token_file,
            (os.path.join(os.environ.get("HF_HOME", ""), "token") if os.environ.get("HF_HOME") else None),
            os.path.expanduser("~/.huggingface/token"),
            os.path.expanduser("~/.cache/huggingface/token"),
        ]
        for p in token_candidates:
            if not p:
                continue
            try:
                tok = Path(p).read_text(encoding="utf-8").splitlines()[0].strip()
                if tok:
                    hf_token = tok
                    break
            except Exception:
                continue
    # è‹¥æ²¡æœ‰æ˜¾å¼ tokenï¼Œåˆ™ç”¨ token=True å°è¯•è¯»å–æœ¬æœºå·²ç™»å½•å‡­è¯ï¼ˆhuggingface-cli loginï¼‰
    token_arg = hf_token if hf_token else True

    tokenizer = AutoTokenizer.from_pretrained(
        args.base_model,
        trust_remote_code=True,
        token=token_arg,
    )
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # è®­ç»ƒï¼ˆå« DDP/torchrunï¼‰ä¸è¦ç”¨ device_map="auto"ï¼Œè®© Trainer/Accelerate æ¥ç®¡è®¾å¤‡æ”¾ç½®
    if args.init_from_config:
        cfg = AutoConfig.from_pretrained(args.base_model, trust_remote_code=True, token=token_arg)
        model = AutoModelForCausalLM.from_config(cfg, trust_remote_code=True)
    else:
        model = AutoModelForCausalLM.from_pretrained(
            args.base_model,
            trust_remote_code=True,
            token=token_arg,
        )
    model.config.use_cache = False
    model.config.pad_token_id = tokenizer.pad_token_id
    if args.gradient_checkpointing:
        model.gradient_checkpointing_enable()
    
    print("ğŸ”§ Preparing LoRA config...")
    target_modules = [m.strip() for m in (args.lora_target_modules or "").split(",") if m.strip()]
    if not target_modules:
        raise ValueError("--lora_target_modules ä¸èƒ½ä¸ºç©ºï¼ˆè‡³å°‘æŒ‡å®šä¸€ä¸ªæ¨¡å—åï¼‰")
    lora_config = LoraConfig(
        r=args.lora_r,
        lora_alpha=args.lora_alpha,
        target_modules=target_modules,
        lora_dropout=args.lora_dropout,
        bias="none",
        task_type="CAUSAL_LM"
    )

    model = get_peft_model(model, lora_config)

    print("ğŸ“¥ Loading SFT dataset...")
    dataset_list = load_sft_dataset(args.data_path)
    print(f"   åŠ è½½äº† {len(dataset_list)} æ¡è®­ç»ƒæ•°æ®")

    if args.max_train_samples and args.max_train_samples > 0:
        dataset_list = dataset_list[: args.max_train_samples]
        print(f"   æµ‹è¯•æ¨¡å¼ï¼šä½¿ç”¨ {len(dataset_list)} æ¡æ•°æ®")

    train_dataset = Dataset.from_list(dataset_list)

    def tokenize_batch(examples):
        return tokenizer(
            examples["text"],
            truncation=True,
            max_length=args.max_length,
            padding=False,  # äº¤ç»™ data collator åŠ¨æ€ paddingï¼Œé¿å… batch stack æŠ¥é”™
        )

    train_dataset = train_dataset.map(tokenize_batch, batched=True, remove_columns=["text"])
    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

    print("ğŸš€ Starting SFT training...")
    if args.bf16 and args.fp16:
        raise ValueError("bf16 å’Œ fp16 ä¸èƒ½åŒæ—¶å¼€å¯ï¼Œè¯·äºŒé€‰ä¸€")

    training_args = TrainingArguments(
        output_dir=output_dir,
        per_device_train_batch_size=args.per_device_train_batch_size,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        num_train_epochs=args.num_train_epochs,
        logging_steps=args.logging_steps,
        save_strategy=args.save_strategy,
        learning_rate=args.learning_rate,
        bf16=args.bf16,
        fp16=args.fp16,
        gradient_checkpointing=args.gradient_checkpointing,
        report_to=("wandb" if args.report_to == "wandb" else []),
        run_name=(args.run_name or f"sft_{timestamp}"),
        max_steps=(args.max_steps if args.max_steps and args.max_steps > 0 else -1),
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        tokenizer=tokenizer,
        data_collator=data_collator,
    )

    trainer.train()

    print("ğŸ’¾ Saving LoRA SFT model...")
    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)

    # è·å–è®­ç»ƒç»“æœ
    final_loss = trainer.state.log_history[-1].get("train_loss") if trainer.state.log_history else None

    # åˆ›å»ºè®­ç»ƒä¿¡æ¯è®°å½•æ–‡ä»¶
    metadata = {
        "training_timestamp": timestamp,
        "training_datetime": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "base_model": args.base_model,
        "data_path": args.data_path,
        "training_config": {
            "epochs": training_args.num_train_epochs,
            "batch_size": training_args.per_device_train_batch_size,
            "gradient_accumulation": training_args.gradient_accumulation_steps,
            "learning_rate": training_args.learning_rate,
            "max_length": args.max_length,
            "lora_r": lora_config.r,
            "lora_alpha": lora_config.lora_alpha,
        },
        "final_loss": final_loss,
        "total_steps": trainer.state.global_step,
        "output_directory": output_dir,
        "files_saved": [
            "adapter_config.json",
            "adapter_model.safetensors",
            "tokenizer.json",
            "tokenizer_config.json",
            "special_tokens_map.json",
            "vocab.json",
            "merges.txt",
            "added_tokens.json",
            "chat_template.jinja",
            "training_metadata.json"
        ]
    }

    metadata_path = os.path.join(output_dir, "training_metadata.json")
    with open(metadata_path, "w", encoding="utf-8") as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)

    if wandb_run is not None:
        import wandb  # noqa: PLC0415

        wandb.log({
            "final_loss": final_loss,
            "total_steps": trainer.state.global_step,
            "training_runtime": trainer.state.log_history[-1].get("train_runtime") if trainer.state.log_history else None,
            "training_config": metadata["training_config"],
        })

        wandb.run.tags = ["sft", "qwen3", "lora"]
        loss_note = f"{final_loss:.4f}" if isinstance(final_loss, (int, float)) else "N/A"
        wandb.run.notes = f"SFT training with {args.base_model}. Final loss: {loss_note}"

    print(f"ğŸ“ Training metadata saved to: {metadata_path}")
    if wandb_run is not None:
        print(f"ğŸ“Š WandB run URL: {wandb.run.url}")
    print("ğŸ‰ SFT training complete!")
