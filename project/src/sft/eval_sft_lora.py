#!/usr/bin/env python3
"""
SFT LoRA æ¨¡å‹è¯„æµ‹è„šæœ¬ï¼ˆæ— é¢å¤–ä¾èµ–ç‰ˆï¼‰

é€‚ç”¨ä½ çš„æ•°æ®æ ¼å¼ï¼ˆjsonlï¼Œæ¯è¡ŒåŒ…å« instruction/input/outputï¼‰ï¼š
  prompt = instruction + ("\n" + input if input else "")
  reference = output

è¯„æµ‹å†…å®¹ï¼š
- ç”Ÿæˆï¼šå¯¹ eval å­é›†åšè´ªå¿ƒ/æŸæœç´¢ç”Ÿæˆ
- æŒ‡æ ‡ï¼šROUGE-1 / ROUGE-2 / ROUGE-Lï¼ˆF1ï¼‰
- è¾“å‡ºï¼šè‹¥å¹²æ¡æ ·ä¾‹å¯¹æ¯” + æ±‡æ€»æŒ‡æ ‡

ç”¨æ³•ç¤ºä¾‹ï¼š
  python project/src/sft/eval_sft_lora.py \
    --adapter_dir project/models/sft/sft_20260105_1453 \
    --data_path project/data/processed/sft_data.jsonl \
    --eval_size 200 --max_new_tokens 256 --num_print 5
"""

from __future__ import annotations

import argparse
import json
import os
import random
import re
from pathlib import Path
from typing import Dict, List, Tuple

import torch
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer


def resolve_hf_token() -> str | None:
    """
    å°½é‡å¤ç”¨è®­ç»ƒè„šæœ¬çš„ token é€»è¾‘ï¼š
    - ä¼˜å…ˆè¯» HF_TOKEN / HUGGINGFACE_HUB_TOKEN
    - å†å°è¯• HF_TOKEN_FILE / HF_HOME/token / ~/.huggingface/token / ~/.cache/huggingface/token
    - éƒ½æ²¡æœ‰åˆ™è¿”å› Noneï¼ˆå…¬å…±æ¨¡å‹é€šå¸¸ä¸éœ€è¦ tokenï¼›ç§æœ‰/é™æµå†è‡ªè¡Œæä¾› HF_TOKENï¼‰
    """
    hf_token = os.environ.get("HF_TOKEN") or os.environ.get("HUGGINGFACE_HUB_TOKEN")
    if hf_token and hf_token.strip():
        return hf_token.strip()

    token_file = os.environ.get("HF_TOKEN_FILE")
    token_candidates = [
        token_file,
        (os.path.join(os.environ.get("HF_HOME", ""), "token") if os.environ.get("HF_HOME") else None),
        os.path.expanduser("~/.huggingface/token"),
        os.path.expanduser("~/.cache/huggingface/token"),
    ]
    for p in token_candidates:
        if not p:
            continue
        try:
            tok = Path(p).read_text(encoding="utf-8").splitlines()[0].strip()
            if tok:
                return tok
        except Exception:
            continue
    return None


def maybe_token_kwargs() -> dict:
    tok = resolve_hf_token()
    return {"token": tok} if tok else {}


def _tokenize_for_rouge(text: str) -> List[str]:
    # æ–°é—»ç”Ÿæˆä»»åŠ¡ï¼šåšä¸€ä¸ªè½»é‡ tokenizationï¼ˆåªç”¨äº ROUGE è¿‘ä¼¼ï¼‰
    # - lower
    # - ä¿ç•™å­—æ¯/æ•°å­—ï¼Œå…¶ä»–å½“åˆ†éš”ç¬¦
    text = text.lower()
    return re.findall(r"[a-z0-9]+", text)


def _ngram_counts(tokens: List[str], n: int) -> Dict[Tuple[str, ...], int]:
    counts: Dict[Tuple[str, ...], int] = {}
    if n <= 0 or len(tokens) < n:
        return counts
    for i in range(len(tokens) - n + 1):
        ng = tuple(tokens[i : i + n])
        counts[ng] = counts.get(ng, 0) + 1
    return counts


def _f1_overlap(pred_counts: Dict[Tuple[str, ...], int], ref_counts: Dict[Tuple[str, ...], int]) -> float:
    if not pred_counts or not ref_counts:
        return 0.0
    overlap = 0
    pred_total = 0
    ref_total = 0
    for k, v in pred_counts.items():
        pred_total += v
        overlap += min(v, ref_counts.get(k, 0))
    for v in ref_counts.values():
        ref_total += v
    if pred_total == 0 or ref_total == 0 or overlap == 0:
        return 0.0
    p = overlap / pred_total
    r = overlap / ref_total
    return 2 * p * r / (p + r) if (p + r) > 0 else 0.0


def _lcs_len(a: List[str], b: List[str]) -> int:
    # æ ‡å‡† DPï¼šO(len(a)*len(b))ï¼Œeval_size ä¸å¤§æ—¶è¶³å¤Ÿ
    if not a or not b:
        return 0
    n, m = len(a), len(b)
    prev = [0] * (m + 1)
    for i in range(1, n + 1):
        cur = [0] * (m + 1)
        ai = a[i - 1]
        for j in range(1, m + 1):
            if ai == b[j - 1]:
                cur[j] = prev[j - 1] + 1
            else:
                cur[j] = max(prev[j], cur[j - 1])
        prev = cur
    return prev[m]


def rouge_f1(pred: str, ref: str) -> Dict[str, float]:
    pred_toks = _tokenize_for_rouge(pred)
    ref_toks = _tokenize_for_rouge(ref)
    r1 = _f1_overlap(_ngram_counts(pred_toks, 1), _ngram_counts(ref_toks, 1))
    r2 = _f1_overlap(_ngram_counts(pred_toks, 2), _ngram_counts(ref_toks, 2))
    lcs = _lcs_len(pred_toks, ref_toks)
    if not pred_toks or not ref_toks or lcs == 0:
        rl = 0.0
    else:
        p = lcs / len(pred_toks)
        r = lcs / len(ref_toks)
        rl = 2 * p * r / (p + r) if (p + r) > 0 else 0.0
    return {"rouge1_f1": r1, "rouge2_f1": r2, "rougeL_f1": rl}


def load_jsonl(path: str) -> List[dict]:
    items: List[dict] = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            items.append(json.loads(line))
    return items


def build_prompt(item: dict) -> str:
    instruction = item.get("instruction", "")
    inp = item.get("input", "")
    return instruction + (("\n" + inp) if inp else "")


def resolve_base_model(adapter_dir: str, base_model_arg: str | None) -> str:
    if base_model_arg:
        return base_model_arg
    meta = Path(adapter_dir) / "training_metadata.json"
    if meta.exists():
        try:
            j = json.loads(meta.read_text(encoding="utf-8"))
            bm = j.get("base_model")
            if isinstance(bm, str) and bm.strip():
                return bm.strip()
        except Exception:
            pass
    # å…œåº•ï¼šå°½é‡ä¸çŒœï¼Œæç¤ºç”¨æˆ·æ˜¾å¼ç»™
    raise ValueError("æ— æ³•ä» training_metadata.json æ¨æ–­ base_modelï¼Œè¯·ä¼  --base_model")


def load_model_and_tokenizer(base_model: str, adapter_dir: str):
    dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32
    tokenizer = AutoTokenizer.from_pretrained(adapter_dir, trust_remote_code=True, **maybe_token_kwargs())
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    base = AutoModelForCausalLM.from_pretrained(
        base_model,
        trust_remote_code=True,
        torch_dtype=dtype,
        device_map="auto" if torch.cuda.is_available() else None,
        low_cpu_mem_usage=True,
        **maybe_token_kwargs(),
    )
    model = PeftModel.from_pretrained(base, adapter_dir)
    model.eval()
    return model, tokenizer


def load_base_only(base_model: str):
    dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32
    tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True, **maybe_token_kwargs())
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    model = AutoModelForCausalLM.from_pretrained(
        base_model,
        trust_remote_code=True,
        torch_dtype=dtype,
        device_map="auto" if torch.cuda.is_available() else None,
        low_cpu_mem_usage=True,
        **maybe_token_kwargs(),
    )
    model.eval()
    return model, tokenizer


@torch.no_grad()
def generate_one(
    model,
    tokenizer,
    prompt: str,
    max_new_tokens: int,
    num_beams: int,
    do_sample: bool,
    temperature: float,
    top_p: float,
) -> str:
    inputs = tokenizer(prompt, return_tensors="pt")
    if torch.cuda.is_available():
        inputs = {k: v.to(model.device) for k, v in inputs.items()}

    gen_ids = model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        do_sample=do_sample,
        num_beams=num_beams,
        pad_token_id=tokenizer.pad_token_id,
        eos_token_id=tokenizer.eos_token_id,
        **({"temperature": temperature, "top_p": top_p} if do_sample else {}),
    )
    text = tokenizer.decode(gen_ids[0], skip_special_tokens=True)
    # å»æ‰ prompt å‰ç¼€ï¼Œé¿å…è¯„æµ‹æŠŠæç¤ºè¯ä¹Ÿç®—è¿›å»
    if text.startswith(prompt):
        text = text[len(prompt) :]
    return text.strip()


def main():
    ap = argparse.ArgumentParser(description="SFT LoRA è¯„æµ‹ï¼ˆROUGEï¼Œæ— é¢å¤–ä¾èµ–ï¼‰")
    ap.add_argument("--adapter_dir", type=str, required=True, help="è®­ç»ƒè¾“å‡ºç›®å½•ï¼šåŒ…å« adapter_model.safetensors ç­‰")
    ap.add_argument("--data_path", type=str, default="project/data/processed/sft_data.jsonl")
    ap.add_argument("--base_model", type=str, default=None, help="ä¸ä¼ åˆ™å°è¯•ä» training_metadata.json æ¨æ–­")
    ap.add_argument("--no_lora", action="store_true", help="åªè¯„æµ‹ base_modelï¼ˆä¸åŠ è½½ LoRAï¼‰ï¼Œç”¨äºåš before/after å¯¹æ¯”")

    ap.add_argument("--seed", type=int, default=42)
    ap.add_argument("--eval_size", type=int, default=200, help="è¯„æµ‹æ ·æœ¬æ•°ï¼ˆä»å…¨é‡æ•°æ®éšæœºæŠ½æ ·ï¼‰")
    ap.add_argument("--num_print", type=int, default=5, help="æ‰“å°å¤šå°‘æ¡å¯¹æ¯”æ ·ä¾‹")
    ap.add_argument("--cpu_threads", type=int, default=0, help="CPU æ¨ç†çº¿ç¨‹æ•°ï¼ˆ0 è¡¨ç¤ºä¸è®¾ç½®ï¼‰")
    ap.add_argument("--dtype", type=str, default="auto", choices=["auto", "fp32", "bf16"], help="æƒé‡ dtypeï¼›æ— å¡å»ºè®® fp32ï¼ˆç¨³ï¼‰æˆ– bf16ï¼ˆçœå†…å­˜ï¼Œè§† CPU æ”¯æŒï¼‰")

    ap.add_argument("--max_new_tokens", type=int, default=256)
    ap.add_argument("--num_beams", type=int, default=1)
    ap.add_argument("--do_sample", action="store_true", help="å¼€å¯é‡‡æ ·ï¼ˆé»˜è®¤å…³é—­ï¼Œè¯„æµ‹æ›´ç¨³å®šï¼‰")
    ap.add_argument("--temperature", type=float, default=0.7)
    ap.add_argument("--top_p", type=float, default=0.9)

    args = ap.parse_args()

    random.seed(args.seed)
    if args.cpu_threads and args.cpu_threads > 0:
        torch.set_num_threads(args.cpu_threads)

    adapter_dir = args.adapter_dir
    if not os.path.exists(adapter_dir):
        raise FileNotFoundError(f"adapter_dir ä¸å­˜åœ¨ï¼š{adapter_dir}")

    base_model = resolve_base_model(adapter_dir, args.base_model)
    print(f"ğŸ”§ base_model: {base_model}")
    print(f"ğŸ”§ adapter_dir: {adapter_dir}")
    print(f"ğŸ“¥ loading data: {args.data_path}")
    items = load_jsonl(args.data_path)
    if not items:
        raise ValueError("æ•°æ®ä¸ºç©º")

    eval_size = min(max(args.eval_size, 1), len(items))
    eval_items = random.sample(items, eval_size)
    print(f"ğŸ§ª eval_size: {eval_size} / {len(items)}")

    # dtype é€‰æ‹©ï¼ˆä¸»è¦ç»™æ— å¡èŠ‚çœå†…å­˜ç”¨ï¼‰
    if args.dtype != "auto":
        if args.dtype == "fp32":
            torch.set_default_dtype(torch.float32)
        elif args.dtype == "bf16":
            torch.set_default_dtype(torch.bfloat16)

    if args.no_lora:
        print("ğŸ§± æ¨¡å¼ï¼šbase_model onlyï¼ˆno_loraï¼‰")
        model, tokenizer = load_base_only(base_model)
    else:
        print("ğŸ§© æ¨¡å¼ï¼šbase_model + LoRA adapter")
        model, tokenizer = load_model_and_tokenizer(base_model, adapter_dir)

    sums = {"rouge1_f1": 0.0, "rouge2_f1": 0.0, "rougeL_f1": 0.0}
    shown = 0
    for idx, it in enumerate(eval_items, start=1):
        prompt = build_prompt(it)
        ref = (it.get("output") or "").strip()
        pred = generate_one(
            model=model,
            tokenizer=tokenizer,
            prompt=prompt,
            max_new_tokens=args.max_new_tokens,
            num_beams=args.num_beams,
            do_sample=args.do_sample,
            temperature=args.temperature,
            top_p=args.top_p,
        )
        m = rouge_f1(pred, ref)
        for k in sums:
            sums[k] += m[k]

        if shown < args.num_print:
            shown += 1
            print("\n" + "=" * 88)
            print(f"[{shown}/{args.num_print}] æ ·ä¾‹ idx={idx}")
            print("- prompt (instruction+summary) -")
            print(prompt)
            print("\n- pred -")
            print(pred)
            print("\n- ref -")
            print(ref)
            print("\n- rouge -")
            print({k: round(v, 4) for k, v in m.items()})

    avg = {k: (sums[k] / eval_size) for k in sums}
    print("\n" + "#" * 88)
    print("âœ… ROUGE (F1) å¹³å‡å€¼ï¼š")
    for k in ["rouge1_f1", "rouge2_f1", "rougeL_f1"]:
        print(f"{k}: {avg[k]:.4f}")
    print("#" * 88)


if __name__ == "__main__":
    main()


